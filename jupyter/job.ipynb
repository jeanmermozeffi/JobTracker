{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T08:41:28.634227Z",
     "start_time": "2024-06-28T08:41:27.273396Z"
    }
   },
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import unidecode\n",
    "import uuid\n",
    "from urllib.parse import urlencode\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# wiijob",
   "id": "f397e50464902aee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Definition des fonctions",
   "id": "19a3b1d986453a63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T08:41:37.802572Z",
     "start_time": "2024-06-28T08:41:37.795265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_location(location_string):\n",
    "    \"\"\"\n",
    "    Fonction pour nettoyer une chaîne de localisation en supprimant les espaces inutiles.\n",
    "    \n",
    "    Args:\n",
    "    - location_string (str): Chaîne de localisation à nettoyer.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Chaîne de localisation nettoyée.\n",
    "    \"\"\"\n",
    "    # Supprimer les espaces inutiles et découper par virgule\n",
    "    cleaned_parts = [part.strip() for part in location_string.split(',')]\n",
    "\n",
    "    # Réassembler la chaîne avec une virgule et un espace\n",
    "    cleaned_location = ', '.join(cleaned_parts)\n",
    "\n",
    "    return cleaned_location"
   ],
   "id": "460ca2621c2f03fd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:07:51.712733Z",
     "start_time": "2024-06-28T10:07:51.705330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_soup(url, params=None):\n",
    "    \"\"\"\n",
    "    Fonction pour récupérer et parser le contenu HTML d'une URL donnée avec BeautifulSoup.\n",
    "    \n",
    "    Args:\n",
    "    - url (str): L'URL de la page à scraper.\n",
    "    \n",
    "    Returns:\n",
    "    - BeautifulSoup object: L'objet BeautifulSoup contenant le contenu HTML parsé.\n",
    "      None si la requête échoue.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_link = url\n",
    "    \n",
    "    if params:\n",
    "        full_params = urlencode(params)\n",
    "        full_link = urljoin(url, '?' + full_params)\n",
    "    \n",
    "    print(full_link)\n",
    "        \n",
    "    # Faire une requête HTTP GET\n",
    "    response = requests.get(full_link)\n",
    "\n",
    "    # Vérifier si la requête s'est bien passée\n",
    "    if response.status_code == 200:\n",
    "        # Analyser le contenu HTML avec BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(f\"Erreur {response.status_code} lors de la requête HTTP.\")\n",
    "        return None"
   ],
   "id": "bc3be83d1f57c941",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T09:51:14.772773Z",
     "start_time": "2024-06-28T09:51:14.722741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_jobs(soup):\n",
    "    \"\"\"\n",
    "    Fonction pour récupérer les offres d'emploi à partir d'une URL donnée.\n",
    "    \n",
    "    Args:\n",
    "    - url (str): L'URL de la page d'offres d'emploi.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Une liste de dictionnaires contenant les détails de chaque offre.\n",
    "    \"\"\"\n",
    "    job_list = []\n",
    "\n",
    "    # Vérifier si la requête s'est bien passée\n",
    "    if soup:\n",
    "        # Trouver toutes les offres d'emploi\n",
    "        job_content = soup.find_all('div', class_='jobs-wrapper items-wrapper')\n",
    "        if job_content:\n",
    "            job_items = soup.find_all('div', class_='item-job col-sm-12 col-md-12 col-xs-12 lg-clearfix md-clearfix')\n",
    "\n",
    "            for job_item in job_items:\n",
    "                job_details = {}\n",
    "    \n",
    "                # Titre de l'offre\n",
    "                job_title_tag = job_item.find('h2', class_='job-title')\n",
    "                job_details['title'] = job_title_tag.text.strip()\n",
    "    \n",
    "                # Lien de l'offre\n",
    "                job_details['link'] = job_title_tag.a['href'] if job_title_tag.a else ''\n",
    "    \n",
    "                # Texte du lien\n",
    "                job_details['poste'] = job_title_tag.a.text.strip() if job_title_tag.a else ''\n",
    "    \n",
    "                # Nom de l'employeur\n",
    "                employer_name = job_item.find('h2', class_='employeur_name_joblist').text.strip()\n",
    "                job_details['employer'] = employer_name\n",
    "    \n",
    "                # Catégorie du poste\n",
    "                job_details['category'] = job_item.find('div', class_='job-category with-icon').a.text.strip()\n",
    "    \n",
    "                # Lieu de l'offre\n",
    "                location_string = job_item.find('div', class_='job-location').text.strip()\n",
    "                job_details['location'] = clean_location(location_string)\n",
    "    \n",
    "                # Type de contrat\n",
    "                job_details['contract_type'] = job_item.find('div', class_='job-type with-title').a.text.strip()\n",
    "    \n",
    "                # Date limite\n",
    "                job_details['deadline'] = job_item.find('div', class_='job-deadline with-icon').text.strip()\n",
    "    \n",
    "                # Ajouter les détails de l'offre à la liste\n",
    "                job_list.append(job_details)\n",
    "    \n",
    "    return job_list"
   ],
   "id": "821f5341f51770a6",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:19:58.961277Z",
     "start_time": "2024-06-28T10:19:58.956012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_pagination_links(soup):\n",
    "    \"\"\"\n",
    "    Fonction pour récupérer les liens de pagination à partir d'un objet BeautifulSoup.\n",
    "    \n",
    "    Args:\n",
    "    - soup (BeautifulSoup): L'objet BeautifulSoup contenant le contenu HTML parsé.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Une liste contenant tous les liens de pagination trouvés.\n",
    "    \"\"\"\n",
    "\n",
    "    pagination_links = set()\n",
    "\n",
    "    # Vérifier si l'objet BeautifulSoup est valide\n",
    "    if soup:\n",
    "        # Trouver le conteneur de pagination\n",
    "        pagination_wrapper = soup.find('div', class_='jobs-pagination-wrapper main-pagination-wrapper')\n",
    "\n",
    "        if pagination_wrapper:\n",
    "            # Trouver tous les liens de pagination dans le conteneur\n",
    "            pagination_items = pagination_wrapper.find_all('a', class_='page-numbers')\n",
    "\n",
    "            for item in pagination_items:\n",
    "                link = item.get('href')\n",
    "                if link:\n",
    "                    pagination_links.add(link)\n",
    "\n",
    "    return list(pagination_links)"
   ],
   "id": "f63ae353ad38907a",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:13:58.263927Z",
     "start_time": "2024-06-28T10:13:58.259303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_params = {\n",
    "    'jobs_ppp': 5,\n",
    "    'filter-category': '137',\n",
    "    'filter-location': '368'\n",
    "    #'filter-career_level': 'Stagiaire',\n",
    "    #'filter-cfield-job-mode': 'Sur site',\n",
    "    #'filter-experience': 'Débutant'\n",
    "}"
   ],
   "id": "446d02e667fc904d",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:14:04.241397Z",
     "start_time": "2024-06-28T10:14:00.433390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = 'https://wiijob.com/offres-emploi/'\n",
    "soup = get_soup(url, query_params)"
   ],
   "id": "31455253769d878b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wiijob.com/offres-emploi/?jobs_ppp=5&filter-category=137&filter-location=368\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:02:38.127227Z",
     "start_time": "2024-06-28T10:02:38.103029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if soup:\n",
    "    # Vous pouvez maintenant travailler avec soup pour extraire les données souhaitées\n",
    "    job_titles = soup.find_all('h2', class_='title')\n",
    "    for title in job_titles:\n",
    "        print(title.text.strip())\n",
    "else:\n",
    "    print(\"Impossible de récupérer le contenu HTML.\")"
   ],
   "id": "ded1dec151590727",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOSTEZ VOTRE CARRIÈRE\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:14:09.052514Z",
     "start_time": "2024-06-28T10:14:09.023453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "jobs = get_jobs(soup)\n",
    "len(jobs)"
   ],
   "id": "36593730ae42eb16",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:14:12.171916Z",
     "start_time": "2024-06-28T10:14:12.165895Z"
    }
   },
   "cell_type": "code",
   "source": "jobs[0]",
   "id": "756e09f2905a1909",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Conseiller Sylviculture et Agroforesterie',\n",
       " 'link': 'https://wiijob.com/offre-emploi/conseiller-sylviculture-et-agroforesterie/',\n",
       " 'poste': 'Conseiller Sylviculture et Agroforesterie',\n",
       " 'employer': 'GIZ – Côte d’Ivoire',\n",
       " 'category': 'Conseiller du travail',\n",
       " 'location': \"Côte d'Ivoire\",\n",
       " 'contract_type': 'CDD',\n",
       " 'deadline': '25/06/2024'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:20:04.006286Z",
     "start_time": "2024-06-28T10:20:03.993417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pagination_links = get_pagination_links(soup)\n",
    "for link in pagination_links:\n",
    "    print(link)"
   ],
   "id": "22615a4ab938bc84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wiijob.com/offres-emploi/page/2/?jobs_ppp=5&filter-category=137&filter-location=368\n",
      "https://wiijob.com/offres-emploi/page/3/?jobs_ppp=5&filter-category=137&filter-location=368\n",
      "https://wiijob.com/offres-emploi/page/4/?jobs_ppp=5&filter-category=137&filter-location=368\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6b5ce482fb3c9ae7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
